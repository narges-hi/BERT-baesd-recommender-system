# -*- coding: utf-8 -*-
"""MF_Theta_Step4_CopmressBy_AE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fK3ClxjWA0rVYrJfQq-2KBzlBN_yAEXR
"""

import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam, RMSprop
from keras.layers import Input, Dense, Embedding, Flatten, Dropout, merge, Activation, BatchNormalization, LeakyReLU
from keras.models import Model
from keras.regularizers import l2

import keras
from keras.datasets import mnist
from keras.models import Sequential, Model
from keras.layers import Dense
from keras.optimizers import Adam

#read the plot_vec_after_BERT_1682_768.npy file
from google.colab import files
uploaded = files.upload()

plot_vec=np.load('plot_vec_movilelense1M_Genres_director_title_after_CountVectorize.npy',allow_pickle=True)

plot_vec=plot_vec.astype(np.float32)

print(plot_vec.shape)

print(plot_vec.shape)



from scipy.spatial import distance

for i in range(len(plot_vec)):
  min=10
  d12 = distance.euclidean(plot_vec[1644], plot_vec[i])
  if(d12<2.5):
    print(i,d12)

train, test = train_test_split(plot_vec,
                                 
                                 test_size=0.2,
                                 random_state=999613182)

train=train.astype(np.float32)

"""# Movielense1M"""

from keras import layers

encoding_dim =20


input_img = keras.Input(shape=(3883,))
# "encoded" is the encoded representation of the input
encoded = layers.Dense(encoding_dim, activation='relu')(input_img)
# "decoded" is the lossy reconstruction of the input
decoded = layers.Dense(3883, activation='sigmoid')(encoded)

# This model maps an input to its reconstruction
autoencoder = keras.Model(input_img, decoded)
# This model maps an input to its encoded representation
encoder = keras.Model(input_img, encoded)
encoded_input = keras.Input(shape=(encoding_dim,))
# Retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# Create the decoder model
decoder = keras.Model(encoded_input, decoder_layer(encoded_input))
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

"""# Molielense 100K"""

from keras import layers

# This is the size of our encoded representations
encoding_dim =20

# This is our input image
#input_img = keras.Input(shape=(768,))

input_img = keras.Input(shape=(1682,))
# "encoded" is the encoded representation of the input
encoded = layers.Dense(encoding_dim, activation='relu')(input_img)
# "decoded" is the lossy reconstruction of the input
decoded = layers.Dense(1682, activation='sigmoid')(encoded)
#decoded = layers.Dense(768, activation='sigmoid')(encoded)

# This model maps an input to its reconstruction
autoencoder = keras.Model(input_img, decoded)
# This model maps an input to its encoded representation
encoder = keras.Model(input_img, encoded)
# This is our encoded (32-dimensional) input
encoded_input = keras.Input(shape=(encoding_dim,))
# Retrieve the last layer of the autoencoder model
decoder_layer = autoencoder.layers[-1]
# Create the decoder model
decoder = keras.Model(encoded_input, decoder_layer(encoded_input))
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')



autoencoder.fit(train,train,
                epochs=50,
                batch_size=10,
                shuffle=True,
                validation_data=(test, test))

encoded = encoder.predict(plot_vec)
print(encoded.shape)

np.save('coun_vectorize_Genre_Title_Director_After_AE_3883_20.npy',encoded)
from google.colab import files
files.download('coun_vectorize_Genre_Title_Director_After_AE_3883_20.npy')



print(len(encoded))

print(np.max(encoded))

for i in range(len(encoded)):
  min=10
  d12 = distance.euclidean(encoded[1644], encoded[i])
  if(d12<3):
    print(i,d12)