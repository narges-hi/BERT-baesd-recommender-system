# -*- coding: utf-8 -*-
"""MF_theta_Step5_Mae_RmseFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vnMozKJttD6OT7LV7QbQt5NLwA350YJa
"""

import numpy as np
import pandas as pd



!wget "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
!unzip ml-100k.zip
!ls

!ls ml-100k/
df = pd.read_csv("ml-100k/u.data",sep='\t',names="user_emb_id,movie_emb_id,rating,timestamp".split(","))
len(df.user_emb_id.unique()), len(df.movie_emb_id.unique())
num_users = df['user_emb_id'].unique().max() + 1
num_movies = df['movie_emb_id'].unique().max() + 1
Rating = df.pivot( index = 'user_emb_id', columns ='movie_emb_id',values = 'rating').fillna(0)
print(Rating.shape)

Rating=np.array(Rating)



import numpy
import sys

 
class MF(object):
    
 
    def __init__(self):
        self.Q = None
        self.P = None
        
 
    def fit(self,user_x_product, latent_features_guess=32, learning_rate=0.01, steps=20, regularization_penalty=0.01, convergeance_threshold=0.001):
     
        print ('training model...')
        return self.__factor_matrix(user_x_product, latent_features_guess, learning_rate, steps, regularization_penalty, convergeance_threshold)
 
    def predict_instance(self, row_index):
       
        return numpy.dot(self.P[row_index, :], self.Q.T)
 
    def predict_all(self):
       
        return numpy.dot(self.P, self.Q.T)
 
    def get_models(self):
       
        return self.P, self.Q
 
    def __factor_matrix(self, R, K, alpha, steps, beta, error_limit):
        """
        R = user x product matrix
        K = latent features count (how many features we think the model should derive)
        alpha = learning rate
        beta = regularization penalty (minimize over/under fitting)
        step = logistic regression steps
        error_limit = algo finishes when error reaches this level
        Returns:
        P = User x features matrix. (How strongly a user is associated with a feature)
        Q = Product x feature matrix. (How strongly a product is associated with a feature)
        To predict, use dot product of P, Q
        """
        # Transform regular array to numpy array
        print("K=",K)
        R = numpy.array(R)
 
        # Generate P - N x K
        # Use random values to start. Best performance
        N = len(R)
        M = len(R[0])
        P = numpy.random.rand(N, K)
 
        # Generate Q - M x K
        # Use random values to start. Best performance
        Q = numpy.random.rand(M, K)
        Q = Q.T
        error = 0
 
        # iterate through max # of steps
        for step in range(steps):
 
            # iterate each cell in r
            for i in range(len(R)):
                for j in range(len(R[i])):
                    if R[i][j] > 0:                   
                        # get the eij (error) side of the equation
                        eij = R[i][j] - numpy.dot(P[i, :], Q[:, j])

                        for k in range(K):
                            # (*update_rule) update pik_hat
                            P[i][k] = P[i][k] + alpha * (2* eij * Q[k][j] - beta * P[i][k]) 
                            # (*update_rule) update qkj_hat
                            #Q[k][j] = Q[k][j] + alpha * ( 2 * eij * P[i][k] - beta * Q[k][j] )
                            Q[k][j] = Q[k][j] + alpha * (  2* eij * P[i][k] - beta * (Q[k][j]))
          
        # track Q, P (learned params)
        # Q = Products x feature strength
        # P = Users x feature strength
        self.Q = Q.T
        self.P = P
 
      # self.__print_fit_stats(error, N, M)
 
    def __error(self, R, P, Q, K, beta):
        """
        Calculates the error for the function
        :param R:
        :param P:
        :param Q:
        :param K:
        :param beta:
        :return:
        """
        e = 0
        for i in range(len(R)):
            for j in range(len(R[i])):
                if R[i][j] > 0:
                   # i=i+1
                 #   j=j+1
                    # loss function error sum( (y-y_hat)^2 )
                    e = e + pow(R[i][j]-numpy.dot(P[i,:],Q[:,j]), 2)
 
                    # add regularization
                    for k in range(K):
                     #   k=k+1
 
                        # error + ||P||^2 + ||Q||^2
                        e = e + (beta/2) * ( pow(P[i][k], 2) + pow(Q[k][j], 2) )
        return e
 
    def rmse(self,R):
        """
        A function to compute the total mean square error
        """
        xs, ys = R.nonzero()
        predicted = self.predict_all()
        count=0
        error = 0
        for x, y in zip(xs, ys):
            count=count+1
            error += pow((R[x, y] - predicted[x, y]), 2)
        return np.sqrt(error/count)
    def mae(self,R):
 
      xs, ys = R.nonzero()
      predicted = self.predict_all()
      count=0
      error = 0
      for x, y in zip(xs, ys):
 
        count=count+1
        error += abs((R[x, y] - predicted[x, y]))
      return (error/count)
    def __print_fit_stats(self, error, samples_count, products_count):
        print ('training complete...')
        print ('------------------------------')
        print ('Stats:')
        print ('Error: %0.2f' % error)
        print ('Samples: ' + str(samples_count))
        print ('Products: ' + str(products_count))
        print ('------------------------------')

class MF_Theta(object):
    
 
    def __init__(self):
        self.Q = None
        self.P = None
        
 
    def fit(self,user_x_product,theta, latent_features_guess=32, learning_rate=0.01, steps=20, regularization_penalty=0.1, convergeance_threshold=0.001):
     
        print ('training model...')
        return self.__factor_matrix(user_x_product,theta, latent_features_guess, learning_rate, steps, regularization_penalty, convergeance_threshold)
 
    def predict_instance(self, row_index):
       
        return numpy.dot(self.P[row_index, :], self.Q.T)
 
    def predict_all(self):
       
        return numpy.dot(self.P, self.Q.T)
 
    def get_models(self):
       
        return self.P, self.Q
 
    def __factor_matrix(self, R,theta, K, alpha, steps, beta, error_limit):
        """
        R = user x product matrix
        K = latent features count (how many features we think the model should derive)
        alpha = learning rate
        beta = regularization penalty (minimize over/under fitting)
        step = logistic regression steps
        error_limit = algo finishes when error reaches this level
        Returns:
        P = User x features matrix. (How strongly a user is associated with a feature)
        Q = Product x feature matrix. (How strongly a product is associated with a feature)
        To predict, use dot product of P, Q
        """
        # Transform regular array to numpy array
        R = numpy.array(R)
 
        # Generate P - N x K
        # Use random values to start. Best performance
        N = len(R)
        M = len(R[0])
        P = numpy.random.rand(N, K)
 
        # Generate Q - M x K
        # Use random values to start. Best performance
        Q = numpy.random.rand(M, K)
        Q = Q.T
        error = 0
 
        # iterate through max # of steps
        for step in range(steps):
 
            # iterate each cell in r
            for i in range(len(R)):
                for j in range(len(R[i])):
                    if R[i][j] > 0:                   
                        # get the eij (error) side of the equation
                        eij = R[i][j] - numpy.dot(P[i, :], Q[:, j])

                        for k in range(K):
                            # (*update_rule) update pik_hat
                            P[i][k] = P[i][k] + alpha * ( 2 *eij * Q[k][j]-beta * P[i][k]) 
                            # (*update_rule) update qkj_hat
                            Q[k][j] = Q[k][j] + alpha * (  2 *eij * P[i][k] - beta * (Q[k][j]-theta[k][j] ))
          
        self.Q = Q.T
        self.P = P
 
 
  
 
    def rmse(self,R):
        """
        A function to compute the total mean square error
        """
        xs, ys = R.nonzero()
        predicted = self.predict_all()
        count=0
        error = 0
        for x, y in zip(xs, ys):
            count=count+1
            error += pow((R[x, y] - predicted[x, y]), 2)
        return np.sqrt(error/count)
    def mae(self,R):
 
      xs, ys = R.nonzero()
      predicted = self.predict_all()
      count=0
      error = 0
      for x, y in zip(xs, ys):
 
        count=count+1
        error += abs((R[x, y] - predicted[x, y]))
      return (error/count)
    def __print_fit_stats(self, error, samples_count, products_count):
        print ('training complete...')
        print ('------------------------------')
        print ('Stats:')
        print ('Error: %0.2f' % error)
        print ('Samples: ' + str(samples_count))
        print ('Products: ' + str(products_count))
        print ('------------------------------')

#read 1682_32 file after AE Bert 'plot_vec_AE_1682_32'
from google.colab import files
uploaded = files.upload()

theta=np.load('plot_vec_AE_1682_20.npy',allow_pickle=True)

print(np.max(theta))

theta=theta.T
theta=theta/(100*(np.max(theta)))

print(theta.shape)

#theta=theta/(100*(np.max(theta)))
b=MF_Theta()    
b.fit(Rating,theta,latent_features_guess=20,learning_rate=0.01,steps=20,regularization_penalty=0.01)
print(b.mae(Rating))
print(b.rmse(Rating))

theta=theta/(2*(np.max(theta)))
b=MF_Theta()    
b.fit(Rating,theta,latent_features_guess=20,learning_rate=0.01,steps=20,regularization_penalty=0.01)
print(b.mae(Rating))
print(b.rmse(Rating))

p,q=b.get_models()
print(p,q)
print(np.mean(p),np.mean(q),np.mean(theta))

a=MF() 
a.fit(Rating,latent_features_guess=20,learning_rate=0.01,steps=20,regularization_penalty=0.01) 
print(a.mae(Rating))
print(a.rmse(Rating))

a=MF() 
a.fit(Rating,latent_features_guess=,learning_rate=0.01,steps=20,regularization_penalty=0.1) 
print(a.mae(Rating))
print(a.rmse(Rating))

c=MF_Theta()    
c.fit(Rating,theta,latent_features_guess=5,learning_rate=0.01,steps=50,regularization_penalty=0.1)
print(c.mae(Rating))
print(c.rmse(Rating))

d=MF() 
d.fit(Rating,latent_features_guess=5,learning_rate=0.01,steps=50,regularization_penalty=0.1) 
print(d.mae(Rating))
print(d.rmse(Rating))

#Set the number of values to replace. For example 20%:
prop = int(R.size * 0.2)
print("R.size",R.size)
#Randomly choose indices of the numpy array:
print(R.shape)
I=[]
J=[]
for k in (range(prop)):
  i=np.random.choice(R.shape[0])
  j=np.random.choice(R.shape[1])
  
  if(R[i,j]>0):       
    R[i,j]=0
    I.append(i)
    J.append(j)
    
#print("Original:\n",R1)
#print("Test Set:\n",R)
R=np.rint(R)
print(I)
print(J)

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(R, R1)

print("MSE=",mse**0.5)

print("\nTraining ...\n")

a = MF_Theta()
a.fit(R,R)

print("\nDone\n")



print(a.mae(R))
print(a.rmse(R))