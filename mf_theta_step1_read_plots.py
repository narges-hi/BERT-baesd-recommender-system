# -*- coding: utf-8 -*-
"""MF_theta_step1_read_plots.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qftd0d0W5f4_396Gg59t_KlY4ARcUOi0
"""

pip install surprise

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import math
import re
import sklearn
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import Reader, Dataset, SVD#, evaluate
sns.set_style("darkgrid")

from cvxpy import *
from numpy import matrix
!pip install imdbpy
import imdb
from imdb import IMDb
import keras
import pandas as pd
import matplotlib.pyplot as plt
import math
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from keras.optimizers import Adam, RMSprop
from keras.layers import Input, Dense, Embedding, Flatten, Dropout, merge, Activation, BatchNormalization, LeakyReLU
from keras.models import Model
from keras.regularizers import l2
import numpy as np
# %matplotlib inline  
import warnings
warnings.filterwarnings('ignore')
from keras.callbacks import EarlyStopping, ModelCheckpoint
from scipy.sparse import csr_matrix
import tensorflow as tf
from tensorflow.python.keras.models import model_from_json
from sklearn import preprocessing
from keras.utils import plot_model
from keras.models import Sequential
import csv
import urllib.parse
import urllib.request
from bs4 import BeautifulSoup

import os.path
import urllib.request
import sklearn.datasets
import numpy as np
import re
import tarfile
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC, SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import PassiveAggressiveClassifier, RidgeClassifier
import nltk
from sklearn.datasets import load_files
nltk.download('stopwords')
import pickle
from nltk.corpus import stopwords
print("Setup Complete\n")

#!wget "http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip"
!unzip BX-CSV-Dump.zip

#!ls BX-CSV-Dump/
df = pd.read_csv("BX-CSV-Dump/BX-Books.csv",sep=';')

class MF():

    def __init__(self, R, K, alpha, beta,theta, iterations):
        """
        Perform matrix factorization to predict empty
        entries in a matrix.

        Arguments
        - R (ndarray)   : user-item rating matrix
        - K (int)       : number of latent dimensions
        - alpha (float) : learning rate
        - beta (float)  : regularization parameter
        -theta(as the same size as Q num_item* k): 
        """

        self.R = R
        self.num_users, self.num_items = R.shape
        self.K = K
        self.alpha = alpha
        self.beta = beta
        self.iterations = iterations
        self.theta=theta
    def train(self):
        # Initialize user and item latent feature matrice
        self.P = np.random.normal(scale=1./self.K, size=(self.num_users, self.K))
        self.Q = np.random.normal(scale=1./self.K, size=(self.num_items, self.K))

        # Initialize the biases
        self.b_u = np.zeros(self.num_users)
        self.b_i = np.zeros(self.num_items)
        self.b = np.mean(self.R[np.where(self.R != 0)])

        # Create a list of training samples
        self.samples = [
            (i, j, self.R[i, j])
            for i in range(self.num_users)
            for j in range(self.num_items)
            if self.R[i, j] > 0
        ]

        # Perform stochastic gradient descent for number of iterations
        training_process = []
        for i in range(self.iterations):
            np.random.shuffle(self.samples)
            self.sgd()
            mse = self.mse()
            training_process.append((i, mse))
            #if (i+1) % 100 == 0:
            #    print("Iteration: %d ; error = %.4f" % (i+1, mse))

        return training_process

    def mse(self):
        """
        A function to compute the total mean square error
        """
        xs, ys = self.R.nonzero()
        predicted = self.full_matrix()
        error = 0
        for x, y in zip(xs, ys):
            error += pow(self.R[x, y] - predicted[x, y], 2)
        return np.sqrt(error)
    def sgd(self):
        """
        Perform stochastic graident descent
        """
        for i, j, r in self.samples:
            # Computer prediction and error
            prediction = self.get_rating(i, j)
            e = (r - prediction)

            # Update biases
            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])
            self.b_i[j] += self.alpha * (e - self.beta * self.b_i[j])

            # Update user and item latent feature matrices
            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])
            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * (self.Q[j,:]-theta[j,:]))

    def get_rating(self, i, j):
        """
        Get the predicted rating of user i and item j
        """
        prediction = self.b + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)
        return prediction

    def full_matrix(self):
        """
        Computer the full matrix using the resultant biases, P and Q
        """
        return self.b + self.b_u[:,np.newaxis] + self.b_i[np.newaxis:,] + self.P.dot(self.Q.T)

"""## Movilense 100K"""

!wget "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
!unzip ml-100k.zip
!ls
!ls ml-100k/
df = pd.read_csv("ml-100k/u.data",sep='\t',names="user_emb_id,movie_emb_id,rating,timestamp".split(","))

!wget "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
!unzip ml-1m.zip
!ls
!ls ml-1m/
df = pd.read_csv('ml-1m/ratings.dat', delimiter='::', header=None, names=['user_emb_id', 'movie_emb_id', 'rating', 'timestamp'], usecols=['user_emb_id', 'movie_emb_id', 'rating'], engine='python')

len(df.user_emb_id.unique()), len(df.movie_emb_id.unique())
num_users = df['user_emb_id'].unique().max() + 1
num_movies = df['movie_emb_id'].unique().max() + 1
Rating = df.pivot( index = 'user_emb_id', columns ='movie_emb_id',values = 'rating').fillna(0)
print(Rating.shape)

print(Rating)

k=Rating.to_numpy()
print(k[0])
print(Rating[1])



"""Movilense **1M**"""

err=[]
ia=IMDb()
#Rating2=Rating
X=[]
X1=[]
count=-1
aa=0
row_names = ['movie_id', 'movie_title']
reader = pd.read_csv('ml-1m/movies.dat', delimiter='::', header=None)#,names=row_names)
#with open('ml-1m/movies.dat', 'r', encoding = "ISO-8859-1") as f:
  #  reader = csv.DictReader(f, fieldnames=row_names, delimiter='::')
print(reader)
print(reader[0][0])
print(reader[0][1])
print(reader[0][3882])
for i in range(len(reader)):
    aa=aa+1
    movie_id = reader[0][i]
    movie_title  = reader[1][i]
    if (movie_id!=aa):
     print("movie_id=",movie_id, "  title:" , movie_title,"a=",aa)
     aa=aa+1
    X.append([])
    X[count].append(movie_title)
    domain = 'http://www.imdb.com'
    search_url = domain + '/find?q=' + urllib.parse.quote_plus(movie_title)
    with urllib.request.urlopen(search_url) as response:
        html = response.read()
        soup = BeautifulSoup(html, 'html.parser')
        # Get url of 1st search result
        try:                        
            title = soup.find('table', class_='findList').tr.a['href']
            movie_url = domain + title
            m=ia.search_movie( movie_title)
            a=ia.get_movie(m[0].movieID)
            #print ("a=",a)
            try: 
                # X[count].append(a['plot'][0])                                     
                for director in a['directors']:
                    X[count].append(director['name'])   
                   # print("director=",director)                   
                for genre in a['genres']:
                  X[count].append(genre)   
                  #print(genre)                
                # actor=a['cast']
                #for i in range(len(actor)):                      
                  # X[count].append(actor[i]['name'])
              
            except:
              pass
            
        except AttributeError:
            err.append(movie_id)
          
            pass

"""Read Movielense 100 K"""

err=[]
ia=IMDb()
Rating2=Rating
X=[]
X1=[]
count=-1
row_names = ['movie_id', 'movie_title']
with open('ml-100k/u.item', 'r', encoding = "ISO-8859-1") as f:
    reader = csv.DictReader(f, fieldnames=row_names, delimiter='|')
    for row in reader:
        count=count+1
        movie_id = row['movie_id']
        movie_title = row['movie_title']
        print("movie_id=",movie_id, "  title:" , movie_title)
        X.append([])
        X[count].append(movie_title)
        domain = 'http://www.imdb.com'
        search_url = domain + '/find?q=' + urllib.parse.quote_plus(movie_title)
        with urllib.request.urlopen(search_url) as response:
            html = response.read()
            soup = BeautifulSoup(html, 'html.parser')
            # Get url of 1st search result
            try:                        
                title = soup.find('table', class_='findList').tr.a['href']
                movie_url = domain + title
                m=ia.search_movie( movie_title)
                a=ia.get_movie(m[0].movieID)
                try: 
                   # X[count].append(a['plot'][0])                                     
                    for director in a['directors']:
                       X[count].append(director['name'])                      
                    for genre in a['genres']:
                      X[count].append(genre)                   
                   # actor=a['cast']
                    #for i in range(len(actor)):                      
                     # X[count].append(actor[i]['name'])
                  
                except:
                  pass
                
            except AttributeError:
                err.append(movie_id)
              
                pass
#print(count)

(print(err))



d=[6, 139, 189, 243 ,539,  624, 830 ,861 ,958 ,1056 ,1068,1153,1178,1192,1266,1304,1344,1345,1351,1359,1369,1403,1408,1420,1501,1516,1535,1536,1557,1562,1569,1586,1630,1633,1634,1639,1660,1671]

np.save('Molilense1M_Genres_director_title.npy',X)
from google.colab import files
files.download('Molilense1M_Genres_director_title.npy')

print(X[1300])